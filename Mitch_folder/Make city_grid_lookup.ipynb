{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49f3951-c3b6-49c1-8422-3c7223bb0892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openmeteo-requests\n",
    "%pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b2d4ecd-0d5c-4048-bbe4-9100d7956613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6205b95f-cdad-4c69-bb9a-695109ba27e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0824dbad-09e2-4d6d-a931-178c932e2311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "\n",
    "def get_historical_weather_spark(\n",
    "    locations_df: SparkDataFrame,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    hourly_vars: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch HISTORICAL weather data for multiple locations and return as PySpark DataFrame.\n",
    "    Uses the archive API which goes back to 1940.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    locations_df : SparkDataFrame\n",
    "        PySpark DataFrame with 'latitude' and 'longitude' columns.\n",
    "        Optional 'elevation' column - if provided, will be used; otherwise fetched from API.\n",
    "    start_date : str\n",
    "        Start date in format 'YYYY-MM-DD' (can go back to 1940)\n",
    "    end_date : str\n",
    "        End date in format 'YYYY-MM-DD'\n",
    "    hourly_vars : list, optional\n",
    "        List of hourly variables to fetch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SparkDataFrame with columns including latitude, longitude, elevation_m, and all weather data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure dates are in correct format\n",
    "    start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')\n",
    "    end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')\n",
    "    \n",
    "    if hourly_vars is None:\n",
    "        hourly_vars = [\n",
    "            \"temperature_2m\",\n",
    "            \"relative_humidity_2m\",\n",
    "            \"apparent_temperature\",\n",
    "            \"dew_point_2m\",\n",
    "            \"precipitation\",\n",
    "            \"weather_code\",\n",
    "            \"pressure_msl\",\n",
    "            \"cloud_cover\",\n",
    "            \"wind_speed_10m\",\n",
    "            \"rain\",\n",
    "            \"snow_depth\",\n",
    "            \"snowfall\"\n",
    "        ]\n",
    "    \n",
    "    # Use the ARCHIVE API endpoint for historical data\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    # Check if elevation column exists in input\n",
    "    has_elevation = 'elevation' in locations_df.columns\n",
    "    \n",
    "    # Convert PySpark DataFrame to Pandas for iteration\n",
    "    if has_elevation:\n",
    "        locations_pandas = locations_df.select(\"latitude\", \"longitude\", \"elevation\").toPandas()\n",
    "    else:\n",
    "        locations_pandas = locations_df.select(\"latitude\", \"longitude\").toPandas()\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Process each location\n",
    "    for idx, row in locations_pandas.iterrows():\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "        input_elevation = row.get('elevation', None) if has_elevation else None\n",
    "        \n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": hourly_vars,\n",
    "            \"temperature_unit\": \"fahrenheit\",\n",
    "            \"wind_speed_unit\": \"mph\",\n",
    "            \"precipitation_unit\": \"inch\",\n",
    "        }\n",
    "        \n",
    "        # Add elevation to params if provided\n",
    "        if input_elevation is not None:\n",
    "            params[\"elevation\"] = input_elevation\n",
    "        \n",
    "        try:\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "            response = responses[0]\n",
    "            \n",
    "            # Get actual grid cell coordinates from API response\n",
    "            grid_lat = response.Latitude()\n",
    "            grid_lon = response.Longitude()\n",
    "            elevation = response.Elevation()\n",
    "            \n",
    "            print(f\"Processing location {idx+1}/{len(locations_pandas)}: \"\n",
    "                  f\"Input: ({lat:.4f}, {lon:.4f}) -> Grid: ({grid_lat:.4f}, {grid_lon:.4f}) - Elevation: {elevation:.1f}m\")\n",
    "            \n",
    "            # Process hourly data\n",
    "            hourly = response.Hourly()\n",
    "            \n",
    "            # Create date range\n",
    "            hourly_data = {\n",
    "                \"date\": pd.date_range(\n",
    "                    start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                    end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                    freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                    inclusive=\"left\"\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Add location information (both input and actual grid cell)\n",
    "            hourly_data[\"input_latitude\"] = lat\n",
    "            hourly_data[\"input_longitude\"] = lon\n",
    "            hourly_data[\"grid_latitude\"] = grid_lat\n",
    "            hourly_data[\"grid_longitude\"] = grid_lon\n",
    "            hourly_data[\"elevation_m\"] = elevation\n",
    "            \n",
    "            # Extract all hourly variables dynamically\n",
    "            for i, var_name in enumerate(hourly_vars):\n",
    "                hourly_data[var_name] = hourly.Variables(i).ValuesAsNumpy()\n",
    "            \n",
    "            # Create DataFrame for this location\n",
    "            location_df = pd.DataFrame(data=hourly_data)\n",
    "            all_data.append(location_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing location ({lat}, {lon}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all location data\n",
    "    if all_data:\n",
    "        combined_pandas_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Convert to PySpark DataFrame\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        spark_df = spark.createDataFrame(combined_pandas_df)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Successfully processed: {len(all_data)}/{len(locations_pandas)} locations\")\n",
    "        print(f\"Total records: {spark_df.count()}\")\n",
    "        print(f\"Date range: {start_date} to {end_date}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return spark_df\n",
    "    else:\n",
    "        raise ValueError(\"No data was successfully retrieved for any location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b676afb2-54b3-4aa9-a4b1-10ac8ebb32db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cities = spark.table('workspace.idasky.idaho_cities')\n",
    "\n",
    "display(cities.limit(5))\n",
    "\n",
    "cities.printSchema()\n",
    "\n",
    "cities.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74b4c37-a8ca-4a0f-a7fe-b47f6724880a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "\n",
    "def create_city_grid_connection(cities_df: SparkDataFrame):\n",
    "    \"\"\"\n",
    "    Create a connection table mapping cities to their weather grid cells.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cities_df : SparkDataFrame\n",
    "        Cities DataFrame with 'lat' and 'long' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SparkDataFrame with columns:\n",
    "        - city_lat: original city latitude\n",
    "        - city_long: original city longitude  \n",
    "        - grid_lat: weather grid latitude\n",
    "        - grid_long: weather grid longitude\n",
    "        - grid_elevation: elevation of the grid cell in meters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the ARCHIVE API endpoint\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    test_date = \"2025-12-20\"\n",
    "    \n",
    "    # Convert to Pandas\n",
    "    cities_pandas = cities_df.select(\"lat\", \"long\").toPandas()\n",
    "    \n",
    "    connection_data = []\n",
    "    \n",
    "    # Process each city location\n",
    "    for idx, row in cities_pandas.iterrows():\n",
    "        city_lat = row['lat']\n",
    "        city_long = row['long']\n",
    "        \n",
    "        params = {\n",
    "            \"latitude\": city_lat,\n",
    "            \"longitude\": city_long,\n",
    "            \"start_date\": test_date,\n",
    "            \"end_date\": test_date,\n",
    "            \"hourly\": [\"temperature_2m\"],  # Minimal data just to get grid info\n",
    "            \"temperature_unit\": \"fahrenheit\",\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "            response = responses[0]\n",
    "            \n",
    "            # Get actual grid cell info from API\n",
    "            grid_lat = response.Latitude()\n",
    "            grid_long = response.Longitude()\n",
    "            grid_elevation = response.Elevation()\n",
    "            \n",
    "            print(f\"Processing {idx+1}/{len(cities_pandas)}: \"\n",
    "                  f\"City ({city_lat:.4f}, {city_long:.4f}) -> \"\n",
    "                  f\"Grid ({grid_lat:.4f}, {grid_long:.4f}) @ {grid_elevation:.1f}m\")\n",
    "            \n",
    "            connection_data.append({\n",
    "                \"city_lat\": city_lat,\n",
    "                \"city_long\": city_long,\n",
    "                \"grid_lat\": grid_lat,\n",
    "                \"grid_long\": grid_long,\n",
    "                \"grid_elevation\": grid_elevation\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ({city_lat}, {city_long}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create connection DataFrame\n",
    "    if connection_data:\n",
    "        connection_pandas = pd.DataFrame(connection_data)\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        connection_spark = spark.createDataFrame(connection_pandas)\n",
    "        \n",
    "        unique_grids = connection_spark.select(\"grid_lat\", \"grid_long\").distinct().count()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Successfully mapped: {len(connection_data)} cities\")\n",
    "        print(f\"Unique grid cells: {unique_grids}\")\n",
    "        print(f\"API call savings: {len(connection_data) - unique_grids} calls\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return connection_spark\n",
    "    else:\n",
    "        raise ValueError(\"No connection data was retrieved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0cde0f-df75-47ec-93c7-4927963c456c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the connection table\n",
    "cities = spark.table('workspace.idasky.idaho_cities')\n",
    "connection = create_city_grid_connection(cities)\n",
    "\n",
    "# Show the connection table\n",
    "connection.show(20)\n",
    "\n",
    "# Show unique grids\n",
    "print(\"\\nUnique weather grids:\")\n",
    "unique_grids = connection.select(\"grid_lat\", \"grid_long\", \"grid_elevation\").distinct()\n",
    "unique_grids.show(20)\n",
    "print(f\"Total unique grids: {unique_grids.count()}\")\n",
    "\n",
    "# Save the connection table\n",
    "# connection.write.mode(\"overwrite\").saveAsTable(\"workspace.idasky.city_to_weather_grid\")\n",
    "print(\"\\nConnection table saved to: workspace.idasky.city_to_weather_grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036505ce-2afc-4b0a-9b05-ab4a2e338392",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766374873088}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766374927853}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# unique_grids = connection.select(\"grid_lat\", \"grid_long\").distinct()\n",
    "# unique_grids.show(20)\n",
    "# print(f\"Total unique grids: {unique_grids.count()}\")\n",
    "# connection.count()\n",
    "\n",
    "display(connection)\n",
    "display(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e37f26-479c-4fdc-bddd-2eb6e593c82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Option 2: More concise\n",
    "connection.write.mode(\"overwrite\").saveAsTable(\"workspace.idasky.city_grid_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e00f9d-c931-4ae5-98b5-83ad108f9d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Rename columns\n",
    "# cities_renamed = cities.withColumnRenamed(\"lat\", \"latitude\").withColumnRenamed(\"long\", \"longitude\")\n",
    "\n",
    "# # Fetch weather data for December 6, 2025 (24 hours)\n",
    "# weather_df = get_historical_weather_spark(\n",
    "#     locations_df=cities_renamed,\n",
    "#     start_date=\"2025-12-06\",\n",
    "#     end_date=\"2025-12-06\",\n",
    "#     hourly_vars=[\n",
    "#         \"temperature_2m\",\n",
    "#         \"relative_humidity_2m\",\n",
    "#         \"apparent_temperature\",\n",
    "#         \"dew_point_2m\",\n",
    "#         \"precipitation\",\n",
    "#         \"weather_code\",\n",
    "#         \"pressure_msl\",\n",
    "#         \"cloud_cover\",\n",
    "#         \"wind_speed_10m\",\n",
    "#         \"rain\",\n",
    "#         \"snow_depth\",\n",
    "#         \"snowfall\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Show sample of weather data\n",
    "# print(\"\\nSample weather data:\")\n",
    "# weather_df.show(10)\n",
    "\n",
    "# # Show unique grid points to see which weather grid cell each city uses\n",
    "# print(\"\\nGrid cells used (city coordinates vs API grid coordinates):\")\n",
    "# weather_df.select(\"latitude\", \"longitude\", \"elevation_m\").distinct().orderBy(\"latitude\", \"longitude\").show(237)\n",
    "\n",
    "# # You can also join back to original cities data to compare\n",
    "# print(\"\\nJoining with original city data to see differences:\")\n",
    "# comparison = cities_renamed.join(\n",
    "#     weather_df.select(\"latitude\", \"longitude\", \"elevation_m\").distinct(),\n",
    "#     on=[\"latitude\", \"longitude\"],\n",
    "#     how=\"left\"\n",
    "# )\n",
    "# comparison.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5041c90d-59f2-4e14-9f0d-92b34c50aee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get weather data\n",
    "weather_df = get_historical_weather_spark(\n",
    "    locations_df=cities_renamed,\n",
    "    start_date=\"2025-12-06\",\n",
    "    end_date=\"2025-12-06\"\n",
    ")\n",
    "\n",
    "# Create a mapping table of cities to grid cells\n",
    "connection = weather_df.select(\n",
    "    \"input_latitude\", \n",
    "    \"input_longitude\",\n",
    "    \"grid_latitude\",\n",
    "    \"grid_longitude\",\n",
    "    \"elevation_m\"\n",
    ").distinct()\n",
    "\n",
    "print(f\"Cities: {cities_renamed.count()}\")\n",
    "print(f\"Unique grid cells: {connection.select('grid_latitude', 'grid_longitude').distinct().count()}\")\n",
    "\n",
    "connection.show(20)\n",
    "\n",
    "# Get unique grid cells to fetch weather for in the future\n",
    "unique_grids = connection.select(\n",
    "    col(\"grid_latitude\").alias(\"latitude\"),\n",
    "    col(\"grid_longitude\").alias(\"longitude\")\n",
    ").distinct()\n",
    "\n",
    "print(f\"\\nUnique grids to fetch: {unique_grids.count()}\")\n",
    "unique_grids.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a777fe8-473c-4399-b02c-b26c9fe4d2b9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766371910290}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(comparison)\n",
    "comparison.select(countDistinct(\"elevation_m\")).show()\n",
    "connection.select(countDistinct(\"grid_latitude\", \"grid_longitude\").alias(\"distinct_grid_pairs\")).show()\n",
    "connection_agg = connection.groupBy('grid_latitude', 'grid_longitude').agg(\n",
    "    count('*').alias('locations_that_use'),\n",
    "    min(col('elevation_m')).alias('min_elevation'),\n",
    "    max(col('elevation_m')).alias('max_elevation')\n",
    ").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6030b7ba-f9a7-462f-9cfc-35e5525e0353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See if any grid cells have multiple elevations (shouldn't happen)\n",
    "connection.groupBy(\"grid_latitude\", \"grid_longitude\").agg(\n",
    "    countDistinct(\"elevation_m\").alias(\"num_elevations\")\n",
    ").filter(\"num_elevations > 1\").show()\n",
    "\n",
    "# Or check the reverse - elevations shared by multiple grid cells\n",
    "connection.groupBy(\"elevation_m\").agg(\n",
    "    countDistinct(\"grid_latitude\", \"grid_longitude\").alias(\"num_grids\")\n",
    ").filter(\"num_grids > 1\").orderBy(\"num_grids\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Make city_grid_lookup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
