{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38438842-81e4-462d-962c-7a65c10ae3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openmeteo-requests\n",
    "%pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0954a761-44d7-42fa-87cd-21f6b484fbaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The purpose of this file is to grab the previous days weather and save those results to the data bricks table for now. Perhaps I will build a staging table and then that table will be pushed to a mysql database in another notebook.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "cities = spark.table('workspace.idasky.idaho_cities')\n",
    "connection = spark.table('workspace.idasky.city_grid_lookup')\n",
    "# connection.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce68535-9f1f-46da-8505-bfb60b6c4fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will let us grab the correct dates from idaho. We only want to grab the previous dates that are completed.\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "idaho_tz = ZoneInfo(\"America/Boise\")  # Most of Idaho\n",
    "\n",
    "now_idaho = datetime.now(tz=idaho_tz)\n",
    "today = now_idaho.strftime(\"%Y-%m-%d\")\n",
    "yesterday = (now_idaho - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Today:\", today)\n",
    "print(\"Yesterday:\", yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62527413-600c-454c-922e-a803d2452627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3eb8b31-ce83-4188-bca6-23e5ef3545fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "\n",
    "def get_yesterday_weather_for_grids(grid_locations_df: SparkDataFrame, hourly_vars: list = None):\n",
    "    \"\"\"\n",
    "    Fetch yesterday's weather data for unique grid locations (Idaho timezone).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_locations_df : SparkDataFrame\n",
    "        DataFrame with 'grid_lat' and 'grid_long' columns\n",
    "    hourly_vars : list, optional\n",
    "        List of hourly variables to fetch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SparkDataFrame with yesterday's hourly weather data for all grid locations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get yesterday's date in Idaho timezone\n",
    "    idaho_tz = ZoneInfo(\"America/Boise\")\n",
    "    now_idaho = datetime.now(tz=idaho_tz)\n",
    "    yesterday = (now_idaho - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Fetching weather data for: {yesterday} (Idaho time)\")\n",
    "    \n",
    "    if hourly_vars is None:\n",
    "        hourly_vars = [\n",
    "            \"temperature_2m\",\n",
    "            \"relative_humidity_2m\",\n",
    "            \"apparent_temperature\",\n",
    "            \"dew_point_2m\",\n",
    "            \"precipitation\",\n",
    "            \"weather_code\",\n",
    "            \"pressure_msl\",\n",
    "            \"cloud_cover\",\n",
    "            \"wind_speed_10m\",\n",
    "            \"rain\",\n",
    "            \"snow_depth\",\n",
    "            \"snowfall\"\n",
    "        ]\n",
    "    \n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    # Get unique grid locations\n",
    "    locations_pandas = grid_locations_df.select(\"grid_lat\", \"grid_long\").distinct().toPandas()\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Process each grid location\n",
    "    for idx, row in locations_pandas.iterrows():\n",
    "        grid_lat = row['grid_lat']\n",
    "        grid_long = row['grid_long']\n",
    "        \n",
    "        params = {\n",
    "            \"latitude\": grid_lat,\n",
    "            \"longitude\": grid_long,\n",
    "            \"start_date\": yesterday,\n",
    "            \"end_date\": yesterday,\n",
    "            \"hourly\": hourly_vars,\n",
    "            \"temperature_unit\": \"fahrenheit\",\n",
    "            \"wind_speed_unit\": \"mph\",\n",
    "            \"precipitation_unit\": \"inch\",\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "            response = responses[0]\n",
    "            \n",
    "            print(f\"Processing grid {idx+1}/{len(locations_pandas)}: ({grid_lat:.4f}, {grid_long:.4f})\")\n",
    "            \n",
    "            # Process hourly data\n",
    "            hourly = response.Hourly()\n",
    "            \n",
    "            hourly_data = {\n",
    "                \"date\": pd.date_range(\n",
    "                    start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                    end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                    freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                    inclusive=\"left\"\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            hourly_data[\"grid_lat\"] = grid_lat\n",
    "            hourly_data[\"grid_long\"] = grid_long\n",
    "            \n",
    "            # Extract all hourly variables\n",
    "            for i, var_name in enumerate(hourly_vars):\n",
    "                hourly_data[var_name] = hourly.Variables(i).ValuesAsNumpy()\n",
    "            \n",
    "            location_df = pd.DataFrame(data=hourly_data)\n",
    "            all_data.append(location_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing grid ({grid_lat}, {grid_long}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if all_data:\n",
    "        combined_pandas_df = pd.concat(all_data, ignore_index=True)\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        spark_df = spark.createDataFrame(combined_pandas_df)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Successfully processed: {len(all_data)}/{len(locations_pandas)} grids\")\n",
    "        print(f\"Total hourly records: {spark_df.count()}\")\n",
    "        print(f\"Date: {yesterday}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return spark_df\n",
    "    else:\n",
    "        raise ValueError(\"No data was successfully retrieved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86cd495-6484-4f12-8f16-82e56199622b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Usage:\n",
    "# # Get unique grids from your connection table\n",
    "connection = spark.table('workspace.idasky.city_grid_lookup')\n",
    "unique_grids = connection.select(\"grid_lat\", \"grid_long\").distinct()\n",
    "\n",
    "# # Fetch yesterday's weather\n",
    "yesterday_weather = get_yesterday_weather_for_grids(unique_grids)\n",
    "\n",
    "# yesterday_weather.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98503112-3e94-497a-ba9b-fcf382d1b4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yesterday_weather = (\n",
    "    yesterday_weather\n",
    "                    .withColumn(\"year\", year(\"date\")) \n",
    "                    .withColumn(\"month\", month(\"date\")) \n",
    "                    .withColumn(\"day\", dayofmonth(\"date\"))\n",
    ")\n",
    "# display(yesterday_weather)\n",
    "\n",
    "# daily_agg = yesterday_weather.groupBy('year', 'month', 'day', 'grid_lat', 'grid_long').agg(\n",
    "#     max('temperature_2m').alias('temp_high'),\n",
    "#     min('temperature_2m').alias('temp_low'),\n",
    "#     max('relative_humidity_2m').alias('humidity_high'),\n",
    "#     min('temperature_2m').alias('humidity_low'),\n",
    "#     sum('rain').alias('total_rain'),\n",
    "#     sum('snowfall').alias('total_snow')\n",
    "# )\n",
    "\n",
    "# display(daily_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6574f6ba-6294-4abb-9c44-cf44798360a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# Add Idaho time columns if needed\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Table name in your schema\n",
    "table_name = \"workspace.idasky.idaho_historic\"\n",
    "\n",
    "# 1️⃣ Create table if it doesn't exist\n",
    "if not spark.catalog.tableExists(table_name):\n",
    "    yesterday_weather.write.format(\"delta\").saveAsTable(table_name)\n",
    "\n",
    "# 2️⃣ Append only new rows (avoid duplicates)\n",
    "else:\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        yesterday_weather.alias(\"source\"),\n",
    "        \"\"\"\n",
    "        target.year = source.year AND\n",
    "        target.month = source.month AND\n",
    "        target.day = source.day AND\n",
    "        target.grid_lat = source.grid_lat AND\n",
    "        target.grid_long = source.grid_long AND\n",
    "        target.date = source.date\n",
    "        \"\"\"\n",
    "    ).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efea620-f6ea-4271-871d-847a316c4640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "historic = spark.table(table_name)\n",
    "historic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38bc42d-773a-45ce-acc0-a174887458f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Table name\n",
    "# table_name = \"workspace.idasky.idaho_historic\"\n",
    "\n",
    "# # Drop the table if it exists\n",
    "# if spark.catalog.tableExists(table_name):\n",
    "#     spark.sql(f\"DROP TABLE {table_name}\")\n",
    "#     print(f\"Table {table_name} has been dropped.\")\n",
    "# else:\n",
    "#     print(f\"Table {table_name} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Daily Weather Grab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
